{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification exercise - housing.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a) Input file\n",
    "import pandas as pd\n",
    "path = 'G:\\My Drive\\CST461 Audit\\housing.txt' #set the path to the datafile\n",
    "data = pd.read_csv(path, header=None, names=['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value','ocean_proximity']) #input the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b) separate features and labels\n",
    "import numpy as np\n",
    "\n",
    "data = data.dropna(axis=0) #dropping the rows where some of the data is na (or nan or null)\n",
    "cols = data.shape[1] #get the number of columns for easy use later\n",
    "features = np.array(data.iloc[:,0:cols-2],np.float32) #split off the feature data into numpy array features\n",
    "rows = features.shape[0] #get the number of rows for easy use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a) convert labels to integers\n",
    "labels = data['ocean_proximity'] #split off the label data into labels\n",
    "#Convert labels to integers\n",
    "labels_strings, labels_ints = np.unique(labels, return_inverse = True)\n",
    "    #I took the above line of code from stackoverflow, and it works.\n",
    "print(labels_strings) #The unique categorical strings\n",
    "print(labels_ints[905:915]) #the integers representing the categorical strings\n",
    "print(labels[905:915]) #the actual labels corresponding to the ints above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2b) convert integer labels to categorical\n",
    "from keras.utils import to_categorical\n",
    "categorical_labels = to_categorical(labels_ints, dtype=\"float32\") #convert to labels and to float32\n",
    "print(categorical_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#3) Separate data into training, validation, and test\n",
    "#The print statements are for debugging.\n",
    "#Import numpy.random to set up the random indices\n",
    "from numpy.random import default_rng\n",
    "#randomize the rows so that when we divide the data we aren't biased toward a particular set\n",
    "rand_idx = default_rng().choice(rows, size = rows, replace=False)\n",
    "features = features[rand_idx] #all th features\n",
    "categorical_labels = categorical_labels[rand_idx] #all the labels\n",
    "#The amount of data that goes into each set is somewhat arbitrary\n",
    "#I've played with this datast a bit, and I don't see a lot of difference from using different size\n",
    "    #train, val, and test sets. So I'll just randomize the size\n",
    "from numpy import random\n",
    "np.random.seed(1000) #I set the random seed in order to get reproducability.\n",
    "small = random.uniform(0,.5) #pick a random float between 0 and .5\n",
    "#That fraction of the data will be evenly divided between the validation and test sets.\n",
    "#The rest will go to training\n",
    "train_size = int((1-small) * rows) #\n",
    "val_size = train_size+int(small/2 * rows)\n",
    "#Set up the feature sets\n",
    "features_train = features[0:train_size,:] #use the biggest chunk for training data\n",
    "features_val = features[train_size:val_size,:] #half of what's left for validation\n",
    "features_test = features[val_size:features.shape[0],:] #and the rest for testing\n",
    "#set up the labels sets\n",
    "categorical_labels_train = categorical_labels[0:train_size]\n",
    "categorical_labels_val = categorical_labels[train_size:val_size]\n",
    "categorical_labels_test = categorical_labels[val_size:features.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Convert features to float32\n",
    "#Check for the types of data in each column\n",
    "#I look at the first element in each column as a proxy for the rest of them\n",
    "for i in range(features.shape[1]):\n",
    "    print(type(features[0,i]))\n",
    "#That says that at least the first value in each column is a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Assemble/create ANN\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "#There are a lot of options here\n",
    "#I've tried a lot of different things here, so I'm not too worried about the number of\n",
    "    #layers or the sizes of them\n",
    "model = models.Sequential()\n",
    "#input dense layer\n",
    "model.add(layers.Dense(33, activation='tanh', input_shape=(8,))) #for whatever reason the input requires a hard coded \n",
    "    #number rather than \"features_train.shape[i]\". Personally I'd prefer the latter, but you do what you have to.\n",
    "#interact with h2\n",
    "model.add(layers.Dense(22, activation='relu'))\n",
    "#interact with h3\n",
    "model.add(layers.Dense(15, activation='tanh'))\n",
    "#interact with h4\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "#output softwax layer for each of the possible classifications.\n",
    "model.add(layers.Dense(5, activation='softmax')) #for whatever reason the input requires a hard coded \n",
    "    #number rather than \"labels_train.shape[j]\". Personally I'd prefer the latter, but you do what you have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Compile network. \n",
    "    #Use standard cross entropy\n",
    "    #rmsprop\n",
    "    #accuracy\n",
    "model.compile(optimizer='rmsprop'\n",
    "              ,loss='categorical_crossentropy'\n",
    "              ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see a summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#7) fit network. \n",
    "    #decide batch size \n",
    "    #num epochs\n",
    "size_batch = random.randint(1024-128)+128 #get a random integer from 128 to 1024\n",
    "print(\"batch size = \", size_batch)\n",
    "num_epochs = random.randint(5)+5 #random integer from 5 to 10\n",
    "print(\"num epochs = \", num_epochs)\n",
    "history=model.fit(features_train,\n",
    "                    categorical_labels_train, epochs=num_epochs, batch_size = size_batch,\n",
    "                    validation_data=(features_val, categorical_labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8) Plot loss and accuracy\n",
    "history_dict = history.history\n",
    "#print(history_dict.keys())\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "test_loss, test_acc = model.evaluate(features_test, categorical_labels_test)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'ro', label = 'Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "print('test_acc:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
